\chapter{Обзор литературы}
\label{sec:Chapter1} \index{Chapter1}


В этой главе представлю итоги анализа наиболее влиятельных и значимых результатов, связанных с играми двух лиц и мультиагентным обучением с подкреплением. Моя цель состоит в том, чтобы представить вам ключевые исследования, которые сформировали основные направления и подходы в данной области. Я уделю внимание на разнообразие методов, применяемых учеными для решения задач и преодоления сложностей, связанных с мультиагентными системами. Этот обзор позволит лучше понять существующие подходы и их взаимосвязь, а также определить перспективы развития данной области науки. Кроме того, я дам формальное описание классической задачи о сделке. Далее в работе будет представлено формальное описание модифицированной задачи о сделке, поэтому важно уже сейчас ознакомиться с классической версией этой экспериментальной игры, чтобы понять различия.


\section{Краткий обзор основных публикаций по теме игр двух лиц и мультиагентного обучения с подкреплением}

В своей статье \cite{nash50} Джон Нэш предложил понятие РН, которое стало фундаментальным в теории игр. РН представляет собой набор стратегий в игре для двух и более игроков, в котором ни один участник не может увеличить выигрыш, изменив свою стратегию, если другие участники своих стратегий не меняют. Нэш доказал существование такого равновесия в смешанных стратегиях в любой конечной игре.

Здесь \cite{littman94} Майкл Литтман предложил марковские игры (стохастические игры) в качестве основы для мультиагентного обучения с подкреплением. Марковские игры являются расширением марковских процессов принятия решений, учитывающих действия нескольких агентов. Автор применил этот подход к кооперативным и конкурентным задачам, демонстрируя его применимость в различных сценариях.

Джералд Тесауро представил TD-Gammon в \cite{tesauro95}. Это программа для игры в нарды, использующую \textbf{TD-обучение} (обучение с подкреплением на основе временной разницы). TD-Gammon стал одним из первых успешных примеров применения обучения с подкреплением в играх и продемонстрировал возможность разработки высокопроизводительных агентов без явных знании. Модель была обучена без знания правил и стратегий игры, а только на основе информации об успехе или неудаче своих ходов. Такой подход показал возможность создания высокопроизводительных агентов без предварительного знания о специфике проблемы, что стало важным шагом в развитии обучения с подкреплением и искусственного интеллекта в целом.

В своей работе \cite{bowling02} Боулинг и Велозо представили алгоритм для мультиагентного обучения с использованием переменной скорости обучения (\textbf{WoLF}). Алгоритм меняет скорость обучения в зависимости от оценки эффективности текущей стратегии агента. Этот подход позволяет агентам лучше адаптироваться к изменяющимся ситуациям в мультиагентных средах.

Обзор \cite{busoniu08} представляет собой широкий анализ методов мультиагентного обучения с подкреплением, включая их теоретические основы, алгоритмы и применение в различных областях. Авторы обсуждают как кооперативные, так и конкурентные сценарии, а также коммуникацию между агентами. Они также рассматривают вопросы сходимости, обучения и адаптации в мультиагентных системах.

В статье \cite{foerster16} авторы представили метод глубокого мультиагентного обучения с подкреплением, который позволяет агентам обучаться совместной кооперативной коммуникации с использованием \textbf{DCC}. Это подход к обучению, в котором агенты обмениваются информацией и настраивают свои стратегии на основе обратной связи других агентов, что позволяет им успешно решать сложные задачи, требующие кооперации.

В работе \cite{lowe17} авторы представили алгоритм \textbf{MADDPG}, который является расширением алгоритма DDPG для мультиагентных сред. MADDPG основан на актор-критике, где каждый агент имеет собственные актор и критик, обучаемые независимо. Он позволяет агентам совместно обучаться в смешанных средах, где они могут взаимодействовать как кооперативно, так и конкурентно. Этот алгоритм продемонстрировал успех в решении сложных мультиагентных задач, таких как управление движением и командные игры.

Статья \cite{legleau20} исследует применение мультиагентного обучения с подкреплением, используя \textbf{Q-обучение} в ультиматумной игре. Авторы применяют Q-обучение для обучения агентов и используют $\epsilon$-жадную стратегию, позволяющую агентам исследовать и эксплуатировать среду. Цель исследования состоит в том, чтобы анализировать и сравнивать различные стратегии агентов, чтобы определить наиболее эффективные подходы в ультиматумной игре.

В статье \cite{chang20} автор применяет глубокое обучение с подкреплением (\textbf{DRL}) для обучения агентов в мультиагентных переговорах. Это исследование направлено на поиск оптимальных стратегий и результатов в сложных переговорах, где участники обсуждают несколько вопросов. Агенты, представленные глубокими нейронными сетями, учатся оптимальной стратегии, основываясь на взаимодействии с окружающей средой и получении подкрепления за свои действия.

\cite{schulman2017ppo} --- статья, в которой представлен метод \textbf{PPO}. Данный метод призван решить некоторые проблемы с обучением с подкреплением, связанные с нестабильностью и большим временем обучения. PPO является методом оптимизации политики, который использует технику, известную как обрезка (англ. clipping), чтобы ограничить обновления политики в каждой итерации оптимизации.

Работа \cite{mnih2013dqn} является одной из первых, в которой обсуждается сочетание глубокого обучения и обучения с подкреплением. Авторы представили алгоритм, который они назвали \textbf{DQN}. Он сочетает Q-обучение, метод обучения с подкреплением, с глубокими нейронными сетями.

В \ref{sec:Chapter3}-ой главе подробно описаны некоторые из упомянутых алгоритмов. Там же выбран алгоритм для решения модифицированной задачи о сделке. Подбор подходящего алгоритма сопровождается подробными комментариями.


\section{Описание существующих подходов к решению задачи о сделке и их ограничения}

\subsection{Задача о сделке}

Задача о сделке --- экспериментальная экономическая игра двух лиц в теории игр, которая моделирует процесс двусторонних переговоров. В этой игре участники должны договориться о распределении конечного объема общих ресурсов. На практике такое взаимодействие возникает в случае, когда торговля может принести прибыль, например, одна из сторон ценит ресурсы меньше. Более того, для моделирования переговоров и анализа такой проблемы необходимо отсутствие установленных ценовых норм на рынке.

Итак, на столе переговоров денежная сумма $M$ --- прибыль от торговли. Игроки ходят поочередно. Суть игры заключается в том, что первый игрок предлагает распределить $M$ в пропорциях $(x, M - x)$. Такой дележ реализуется при наличии согласия второго игрока. В результате его реализации первый получает $x$, второй --- $M - x$. Если игроки не приходят к соглашению, то каждый получает свою часть заранее фиксированного исхода $(a, b)$. Первый получает $a$, второй --- $b$. Следует отметить, что в подавляющем большинстве реальных случаев $a = b = 0$. Предполагается, что $a + b < M$.

Формально задачу о сделке можно представить в виде кортежа $(N, X, u)$, где

\begin{itemize}

\item $N$ --- множество игроков. В этой игре $|N| = 2$, то есть участвуют два игрока, которые обычно называются Игрок 1 и Игрок 2.

\item $X$ --- пространство исходов. $X = \left\{ (x, y) \in \mathbb{R}_{\geq 0}^2: x + y = M \right\}$. Оно является непрерывным и линейным. В этой работе мы не будет нормировать его, то есть не будем рассматривать $\frac{x}{M} + \frac{y}{M} = 1$ вместо $x + y = M$.

\item $u = (u_1, u_2)$ --- пара функций полезности, которая представляет предпочтения каждого игрока относительно результатов в $X$. Функция полезности $u_i: X \rightarrow \mathbb{R}_{\geq 0}$ представляет предпочтения Игрока $i$.

\end{itemize}

Цель игры --- добиться соглашения между участниками о том, как распределить ресурсы. Из-за разногласий в предпочтениях по результатам в $X$ можно наблюдать конфликт интересов. Задача о сделке моделирует этот конфликт как игру стратегического взаимодействия, где каждый игрок пытается договориться о сделке, которая максимизирует его собственную функцию полезности.

Данная задача является важной для понимания того, как агенты принимают решения в условиях конкуренции и ограниченности ресурсов. Ее формализация помогает лучше понять проблемы ведения переговоров в различных экономических ситуациях. Были разработаны различные методы решения этой игры. Одним из распространенных способов решения является решение Нэша. Оно представляет собой набор результатов, которые оба игрока предпочитают любому альтернативному результату. Решение Нэша основано на концепции справедливости, которая предполагает, что оба игрока имеют равную силу в переговорах.\\


\subsection{Решение двухходовой задачи о сделке}

Игра \enquote{Ультиматум} --- двухходовая задача о сделке, описанная выше, является простейшим вариантом задачи о сделке, которая имеет следующее решение: второй игрок примет предложение $(x, M - x)$, если $ M - x \geq b $. Предположим, $ x = M - b $. Если первый игрок изменит свою стратегию с целью максимизации собственного выигрыша, то второму будет выгодно отказать. Кроме того, заметим, что при получении предложения $(M - b, b)$ любая стратегия (принять или отклонить) второго игрока приносит прибыль не больше $b$. Следовательно, исход $(M - b, b)$ является РН. Здесь очевидным недостатком является наличие равновесия $(M, 0)$ при $b = 0$. Таким образом, в реальной ситуации первый агент может оставить себе всю денежную сумму $M$. Формально это будет считаться равновесием. 


\subsection{Переговоры о цене недвижимости}
Пример из реальной жизни:

\begin{itemize}

    \item Продавец не готов продать свою недвижимость меньше $4{,}500{,}000$ рублей.
    
    \item Покупатель готов платить не больше $5{,}000{,}000$ рублей.
        
    \item $M = 5{,}000{,}000-4{,}500{,}000 = 500{,}000, a = b = 0$.
    
    \item Предположим продавец ходит первым и обладает совершенной информацией. Он знает, что покупатель отклонит любое предложение $> 5{,}000{,}000$ и примет любое $\leq 5{,}000{,}000$.
    
    \item Продавец максимизирует свою прибыль предлагая цену равную $5{,}000{,}000$ или $x = 500{,}000$. Покупатель принимает, ведь $M - x \geq b$.
    
    \item Продавец получает $500{,}000$, то есть всю денежную сумму $M$, покупатель --- 0.
    
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2]{simple_bargaining_game_house.jpg}
  \caption{Переговоры о стоимости недвижимости}
\end{figure}

\textbf{Вывод:} покупатель не должен заранее объявить максимальную сумму, которую готов платить за недвижимость. Эту информацию продавец может использовать против него. В том числе из этих соображений я буду рассматривать игру с неполной информацией.


\subsection{Двухходовка с сокращением прибыли}

Предположим, что $M=12$, $\delta=\frac{1}{3}$ — коэффициент дисконтирования. Обратите внимание, что во втором раунде $M=4$. Игрок $2$ знает, что может получить $3.99$ в раунде $2$, поскольку Игрок $1$ предпочтет $0.01$ ничему и примет дележ $(0.01, 3.99)$. Это следует из предположения что игроки ориентированы на будущее, рациональны и максимизируют свой выигрыш. Учитывая это, Игрок $1$ должен предложить Игроку $2$ $4.00 > 3.99$ в первом раунде, оставив себе $12 - 4 = 8$. Поскольку Игрок $2$ понимает, что ни в каком исходе не может получить больше $4.00$, он примет дележ $(8.00, 4.00)$. Предложение Игрока $1$ отдать $4.00$ Игроку $2$ в первом раунде равно сумме ставки в начале финального раунда, умноженной на коэффициент дисконтирования, $\delta \cdot M$. Такое предложение приведет к равновесию. Это обобщается на любую задачу о сделке с $n$ раундами, где $2 < n < \infty$.

Этот пример показывает, что в игровой ситуации, где участники направлены на долгосрочные результаты, действуют рационально и стараются максимизировать свои выигрыши, возможно достижение РН, которое обеспечивает оптимальные исходы. В данном случае, для достижения РН Игрок $1$ в первом раунде предлагает Игроку $2$ условия, которые выгодны обоим, чтобы Игрок $2$ принял предложение и не стремился получить больше в следующем раунде.

\textbf{Вывод:} в задачах о сделке можно достичь РН, которое приводит к эффективному результату, если участники сделки обладают достаточной информацией о возможных исходах, являются рациональными и ориентированы на будущее. Однако, если эти условия не выполняются, могут возникнуть проблемы координации и нежелательные исходы.


\subsection{Переговоры о цене недвижимости (продолжение)}

\begin{itemize}

\item Минимальная стоимость, по которой продавец продаст свою недвижимость составляет $15{,}000{,}000$ рублей, а максимальная цена, которую покупатель готов заплатить --- $16{,}000{,}000$ рублей. Следовательно, $M = 1{,}000{,}000$ рублей.

\item Оба игрока имеют одинаковый коэффициент дисконтирования $\delta=0.8$.

\item Процесс переговоров ограничен двумя раундами. Это обусловлено тем, что продавец обязан продать недвижимость до определенного срока (возможно, для покупки другой недвижимости), а покупателю, в свою очередь, нужно приобрести эту недвижимость до определенной даты.

\item В первом раунде переговоров предложение выдвигает покупатель, а во втором раунде --- продавец.

\item Следует применить метод обратной индукции, начав анализ с последнего, второго раунда переговоров и двигаться в обратном направлении для поиска оптимальной последовательности действии.

\item С точки зрения сегодняшнего дня потенциальный суммарный выигрыш от сделки во втором раунде составляет $\delta \cdot M$. На этом этапе на продавце лежит обязанность предложить контрпредложение.

\item В заключительном раунде продавец предложит оставить себе $\delta \cdot M$, предоставляя покупателю возможность принять или отклонить это предложение. На этом этапе покупателю нет разницы между принятием или отклонением предложения. В обоих случаях его выигрыш равен $0$.

\item Осознавая это, покупатель в первом раунде должен предложить продавцу $\delta \cdot M$. Продавец, будучи равнодушным между ожиданием и немедленным принятием, принимает данное предложение.

\item В нашем примере, где $\delta = 0.8$ и $M = 1{,}000{,}000$, покупатель предлагает продавцу $0.8 \cdot M$ или $800{,}000$, оставляя себе $200{,}000$. Таким образом, стоимость недвижимости составляет $15{,}000{,}000 + 800{,}000 = 15{,}800{,}000$.

\end{itemize}


\subsection{Анализ бесконечно повторяющихся игр}

\begin{itemize}
\item Теперь предположим, что количество переговорных раундов не ограничено. Переговоры могут продолжаться бесконечно.

\item Если покупатель делает предложение первым, то сумма $x(1) \cdot M$, которую он намерен оставить себе в первом раунде, должна гарантировать продавцу приемлемую выгоду. Эта выгода должна быть не меньше той, которую продавец может получить в следующем раунде, если отклонит текущее предложение и предложит забрать $y(2) \cdot M$. Иными словами, в первом раунде продавец должен получить сумму, эквивалентную $\delta \cdot y(2) \cdot M$.

\item Покупатель предлагает продавцу $(1 - x) M = \delta y M$, и таким образом, $x = 1 - \delta y$.

\item Аналогичным образом, продавец должен предложить покупателю $(1 - y) M = \delta x M$. Следовательно, $y = 1 - \delta x$.

\item Получаем систему уравнений:
\begin{equation}
\begin{cases}
    x &= 1 - \delta (1 - \delta x), \\
    y &= 1 - \delta (1 - \delta y).
\end{cases}
\end{equation}

\item $x = y = \frac{1-\delta}{1-\delta^2}$. Обратите внимание, что $x+y > 1$.

\item $x$ и $y$ обозначают суммы, которые получают покупатель и продавец соответственно, если они делают первое предложение в первом раунде.

\end{itemize}


\subsection*{Практические выводы}

\begin{enumerate}

\item В реальной жизни стороны переговоров не знают коэффициенты дисконтирования друг друга или их относительные уровни терпения, но могут пытаться угадать эти значения.

\item Нужно подать сигнал о том, что вы терпеливы, даже если на самом деле нет. Например, не отвечать контрпредложениями сразу же.

\item Более терпеливый игрок получает большую часть суммы $M$, которая находится на столе переговоров.

\end{enumerate}